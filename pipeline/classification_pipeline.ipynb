{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision.models import mobilenet_v3_large\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import shutil\n",
    "from classifier import ImageClassifier\n",
    "import glob\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mobilenet_v3_large()\n",
    "    \n",
    "modules = list(model.children())[:-1]  # delete the last fc layer.\n",
    "model = nn.Sequential(*modules)\n",
    "\n",
    "model.classifier = nn.Sequential(nn.Flatten(),\n",
    "                                nn.Linear(960, 4096),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(4096, 4096),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(4096, 1000),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Dropout(p=0.5),\n",
    "                                nn.Linear(1000, 2),\n",
    "                                nn.Softmax(dim=1)\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "ckpt_path = \"best_acc_epoch=168-val_acc=0.96.ckpt\"\n",
    "best_model = ImageClassifier.load_from_checkpoint(checkpoint_path=ckpt_path, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize(size=(224,224)),\n",
    "        transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_coords(input_array): # finds the coordinates to crop rgb image\n",
    "    bottom = input_array.nonzero()[0].max()\n",
    "    top = input_array.nonzero()[0].min()\n",
    "    \n",
    "    right = input_array.nonzero()[1].max()\n",
    "    left = input_array.nonzero()[1].min()\n",
    "    \n",
    "    return left, top, right, bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(img): # runs the image through classification model\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        transformed_img = img.unsqueeze(dim=0)\n",
    "        output = best_model(transformed_img)\n",
    "    pred_label = torch.argmax(output, dim=1)\n",
    "    \n",
    "    return(pred_label.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_folder(pred, mask_path, person_path): # saves person and mask to desired folder\n",
    "    mask_id = mask_path[18:]\n",
    "    person_id = person_path[18:]\n",
    "    \n",
    "    if pred == 0:\n",
    "        dest_mask = f'.\\\\output\\\\invalid\\\\{mask_id}'\n",
    "        dest_person = f'.\\\\output\\\\invalid\\\\{person_id}'\n",
    "    else:\n",
    "        dest_mask = f'.\\\\output\\\\valid\\\\{mask_id}'\n",
    "        dest_person = f'.\\\\output\\\\valid\\\\{person_id}'\n",
    "    \n",
    "    # create a copy of the mask PNG and person PNG to the respective folder\n",
    "    shutil.copy(mask_path, dest_mask)\n",
    "    shutil.copy(person_path, dest_person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('pedestrian_pool\\\\*_mask*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for i in range(len(files)):\n",
    "    mask = files[i]\n",
    "    rgb = files[i].replace('mask', 'person')\n",
    "    \"\"\"# check to make sure mask and rgb files match\n",
    "    if mask[:-9] == rgb[:-11]:\"\"\"\n",
    "    mask_img = Image.open(mask)\n",
    "    mask_array = np.array(mask_img)\n",
    "    \n",
    "    # check if mask has over 5000 pixels\n",
    "    if np.sum(mask_array == 1) > 5000:\n",
    "        # find number of contours of each mask\n",
    "        mask_cv2 = cv2.imread(mask)\n",
    "        gray = cv2.cvtColor(np.array(mask_cv2), cv2.COLOR_BGR2GRAY)\n",
    "        contours, _ = cv2.findContours(gray, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) \n",
    "        \n",
    "        if len(contours) == 1:\n",
    "\n",
    "            rgb_img = Image.open(rgb)\n",
    "            rgb_img = rgb_img.crop((find_coords(mask_array)))\n",
    "    \n",
    "            # apply transformation and convert to tensor\n",
    "            img = transform(rgb_img)\n",
    "            # evaluate using best model\n",
    "            pred = run_model(img) \n",
    "            # save mask to respective folder\n",
    "            save_to_folder(pred, mask, rgb)\n",
    "        else:\n",
    "            count += 1\n",
    "    else:\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Images <= 1000 Pixels: 6458\n",
      "# of Images > 1000 Pixels: 1192\n"
     ]
    }
   ],
   "source": [
    "print(f'# of Images <= 1000 Pixels: {count}')\n",
    "print(f'# of Images > 1000 Pixels: {len(files)-count}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pedestrian_pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
